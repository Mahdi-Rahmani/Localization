# Lidar odometry
I develop a code that gets lidar scan in each frame as an input and then give us the position of the vehicle. The input scan is a point cloud. From each two consecutive scans we should find the relative displacement. Also we should use a point cloud registration algorithm for obtaining the relative transform matrix between two scans. Here we use ICP point cloud registration. 
For this part first I see the examples of Carla. I use open3d_lidar.py as my base code. you can find it in [Base code](). Also after installing carla on your system, you can run this example code from directory that this simulator is installed. I recommend that you create a seperate environment and install dependencies and python libraries there. I use anaconda for this job and create an environment with the name of carla_test. First you should open a terminal and run carla server side:
``` 
cd C:\CARLA_0.9.12\WindowsNoEditor\
conda activate carla_test
CarlaUE4.exe -carla-rpc-port=3000
```
Becareful I use port=3000 because port=2000 isn't accessible in my computer but in default the carla server port is 2000 and also in examples clients send their requests on port=2000 so you should change them, too if it is needed and if you run above code in terminal. 
Then open another terminal to run open3d_lidar.py in a client mode. For doing this you can run below codes.
``` 
cd C:\CARLA_0.9.12\WindowsNoEditor\PythonAPI\examples\ 
conda activate carla_test
python open3d_lidar.py
``` 

## More about lidar odometry 
Lidar odometry (LO) is a technique used in robotics and autonomous vehicles to estimate the motion of the vehicle by analyzing the data acquired by a Lidar sensor. Lidar sensors use laser beams to measure the distance between the sensor and the surrounding objects, producing a 3D point cloud of the environment. 

LO estimates the motion of the vehicle by comparing consecutive 3D point clouds generated by the Lidar sensor. The basic idea is to find correspondences between points in the two point clouds, and then use these correspondences to estimate the motion of the vehicle. 

The process of finding correspondences is known as feature matching. This involves identifying distinctive features in the point clouds, such as edges or corners, and then searching for the corresponding features in the next point cloud. Once correspondences are found, the motion of the vehicle can be estimated using techniques such as the Iterative Closest Point (ICP) algorithm or the Extended Kalman Filter (EKF). 

LO has several advantages over other odometry techniques, such as wheel odometry, which estimates the motion of the vehicle based on the rotation of the wheels. Lidar odometry is less prone to error caused by wheel slip, and is more accurate in environments with uneven terrain or obstacles. Lidar odometry can also be used in conjunction with other sensors, such as cameras and inertial measurement units (IMUs), to improve accuracy and robustness. 

Lidar odometry has been widely used in robotics and autonomous vehicles, including self-driving cars, unmanned aerial vehicles (UAVs), and mobile robots. However, it has some limitations, such as its dependence on the quality and resolution of the Lidar sensor data, and its sensitivity to changes in the environment, such as lighting conditions and moving objects. Researchers are actively working on developing new techniques and algorithms to overcome these limitations and improve the performance of Lidar odometry. 

## ICP 

ICP (Iterative Closest Point) is a widely used point cloud registration algorithm that aligns two or more point clouds together. The goal of point cloud registration is to find the rigid transformation (i.e., rotation and translation) that aligns the points in one point cloud with the corresponding points in another point cloud. This is useful in many applications such as 3D reconstruction, robot navigation, and augmented reality. 

The basic idea of the ICP algorithm is to iteratively refine an initial estimate of the rigid transformation until the corresponding points in the two point clouds are as close as possible. The algorithm works by first selecting a set of corresponding points (i.e., pairs of points in the two point clouds that are believed to be the same point in the real world) based on some similarity measure, such as Euclidean distance or feature matching. 

Once the corresponding points are selected, the algorithm computes the optimal rigid transformation that aligns the selected points using a least-squares approach. This transformation is then applied to one of the point clouds to bring it into alignment with the other point cloud. This process is repeated until convergence is achieved, which is typically measured by the change in the residual error between the two point clouds. 

There are several variations of the ICP algorithm, each with their own strengths and weaknesses. Some common variants include:

- Point-to-point ICP: This variant uses Euclidean distance to compute the similarity measure between corresponding points. It is simple and fast but can be sensitive to noise and outliers. 

- Point-to-plane ICP: This variant uses the distance between a point and a plane to compute the similarity measure between corresponding points. It is more robust to noise and outliers but can be slower than point-to-point ICP. 

- Robust ICP: This variant uses a robust loss function, such as Huber or Tukey loss, to reduce the influence of outliers on the registration process. It is useful when the point clouds contain a significant amount of noise or outliers. 

ICP has been used successfully in many applications and is considered a fundamental algorithm in the field of point cloud processing. However, it has some limitations, such as its sensitivity to initial alignment and its inability to handle non-rigid deformations. Researchers are actively working on developing new registration algorithms that overcome these limitations and can handle more complex scenarios. 

## My localization procedure 

For doing this job we should do as said in below: 

1- For first and second frames, we know what ground truth is, but from third frame and after that we don't have any external information. 

2- With some Carla's API we can find the position and Transformation matrix of lidar sensor in each moment. so for first and second scan we can get lidar transformation from carla. This matrix transform the lidar coordinate to global coordinate. I call them T1 and T2. so with the help of two lidar transform matrix of scan1 and scan2 find the relative transform matrix (which it transforms lidar pointclouds in second scan to a coordinate attach to first scan location). I call this transform matrix as T_rel_21.

3- Then we can assume constant velocity model as our dynamic model.(because displacement between two consecutive scans is very small so this is a good assumption.) 

4- with the above assumption we can say relative transform between scan3 and scan2 is equal to transform between scan2 and scan1. (T_rel_32 = T_rel_21)

5- As I told befor, we use ICP registration algorithm. This algorithm ask us two point clouds(source and destination pointclouds) and also an initial transform matrix as inputs. This transform matrix is the relative transform between that two input scans(source and destination pointclouds). This algorithm returns the improved transform matrix between those two scans that can match this two pointcloud. 

6- now we know the vehicle position at scan1 and scan2 and we want to calculate the position at scan3. We should use ICP algorithm for this step as below: 
- source = scan2
- destination = scan3
- initial transform = T_rel_32  -> transform matrix that we expected between scan2 and scan3(We calculate it in step 4 in above steps) 

ICP returns us the modified relative transform matrix between scan2 and scan3. (I call it T_rel_32_modified)

7- now we can find position of vehicle in global coordinate according below calculations:
- T3 = T_1 * T_rel_21 * T_rel_32_modified
- pos = T3[:3,-1] 

8- This process can be continued for other scans and find position in each step.